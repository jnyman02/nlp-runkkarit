[
    {
        "title": "Scikit-learn: Machine Learning in Python",
        "authors": [
            "Fabián Pedregosa",
            "Gaël Varoquaux",
            "Alexandre Gramfort",
            "Vincent Michel",
            "Bertrand Thirion",
            "Olivier Grisel",
            "Mathieu Blondel",
            "Peter Prettenhofer",
            "Ron J. Weiss",
            "Vincent Dubourg",
            "Jake Vanderplas",
            "Alexandre Passos",
            "David Cournapeau",
            "Matthieu Brucher",
            "Marc de Perrot",
            "Édouard Duchesnay"
        ],
        "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.",
        "keywords": [
            "MIT License",
            "Python",
            "Robust Learning"
        ]
    },
    {
        "title": "C4.5: Programs for Machine Learning",
        "authors": [
            "J. R. Quinlan"
        ],
        "abstract": "From the Publisher: Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.",
        "keywords": [
            "Learning Classifier Systems",
            "Workstation",
            "Sample (material)"
        ]
    },
    {
        "title": "Pattern Recognition and Machine Learning",
        "authors": [
            "Ian T. Nabney"
        ],
        "abstract": "The <i>Journal of Electronic Imaging</i> (JEI), copublished bimonthly with the Society for Imaging Science and Technology, publishes peer-reviewed papers that cover research and applications in all areas of electronic imaging science and technology.",
        "keywords": [
            "Imaging science",
            "Image Denoising"
        ]
    },
    {
        "title": "Data Mining: Practical Machine Learning Tools and Techniques",
        "authors": [
            "Ian H. Witten",
            "Eibe Frank",
            "Mark A. Hall"
        ],
        "abstract": "Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization",
        "keywords": [
            "Meta-Learning",
            "Data Mining"
        ]
    },
    {
        "title": "Gaussian Processes for Machine Learning",
        "authors": [
            "Carl Edward Rasmussen",
            "Christopher K. I. Williams"
        ],
        "abstract": "A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.",
        "keywords": [
            "Gaussian Processes",
            "Robust Learning",
            "Online machine learning",
            "Relevance vector machine"
        ]
    },
    {
        "title": "Gaussian Processes for Machine Learning",
        "authors": [
            "Carl Edward Rasmussen",
            "Christopher K. I. Williams"
        ],
        "abstract": "A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines. Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.",
        "keywords": [
            "Gaussian Processes",
            "Online machine learning",
            "Backpropagation Learning",
            "Relevance vector machine"
        ]
    },
    {
        "title": "Genetic algorithms in search, optimization, and machine learning",
        "authors": [
            "David E. Goldberg"
        ],
        "abstract": "From the Publisher: This book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. Major concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.",
        "keywords": [
            "Pascal (unit)",
            "Nature-Inspired Algorithms",
            "Optimization Applications",
            "Constraint Handling"
        ]
    },
    {
        "title": "Machine Learning : A Probabilistic Perspective",
        "authors": [
            "Kevin P. Murphy"
        ],
        "abstract": "Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.",
        "keywords": [
            "Data Modeling",
            "Statistical Modeling",
            "Data Analysis",
            "Machine Learning",
            "Visualization",
            "Graphical model",
            "Regularization (linguistics)"
        ]
    },
    {
        "title": "Programs for Machine Learning",
        "authors": [
            "Steven L. Salzberg",
            "Alberto M. Segre"
        ],
        "abstract": "Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students.",
        "keywords": [
            "Successor cardinal",
            "Decision Trees",
            "ID3 algorithm",
            "Machine Learning",
            "Robust Learning",
            "Decision Analysis"
        ]
    },
    {
        "title": "TensorFlow: A system for large-scale machine learning",
        "authors": [
            "Martı́n Abadi",
            "Paul Barham",
            "Jianmin Chen",
            "Zhifeng Chen",
            "Andy Davis",
            "Jay B. Dean",
            "Matthieu Devin",
            "Sanjay Ghemawat",
            "Geoffrey Irving",
            "Michael Isard",
            "Manjunath Kudlur",
            "Josh Levenberg",
            "Rajat Monga",
            "Sherry Moore",
            "Derek G. Murray",
            "Benoit Steiner",
            "Paul A. Tucker",
            "Vijay Vasudevan",
            "Pete Warden",
            "Martin Wicke",
            "Yuan Yu",
            "Xiaoqiang Zheng"
        ],
        "abstract": "TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous parameter server designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",
        "keywords": [
            "GPU Computing",
            "Heterogeneous Computing",
            "High-Performance Computing",
            "Parallel Computing",
            "Deep Learning"
        ]
    },
    {
        "title": "Machine learning in automated text categorization",
        "authors": [
            "Fabrizio Sebastiani"
        ],
        "abstract": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.",
        "keywords": [
            "Software portability",
            "Text categorization",
            "Text Classification",
            "Document Categorization",
            "Text Indexing"
        ]
    },
    {
        "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
        "authors": [
            "Martı́n Abadi",
            "Ashish Agarwal",
            "Paul Barham",
            "Eugene Brevdo",
            "Zhifeng Chen",
            "Craig Citro",
            "Gregory S. Corrado",
            "Andy Davis",
            "Jay B. Dean",
            "Matthieu Devin",
            "Sanjay Ghemawat",
            "Ian Goodfellow",
            "Andrew Harp",
            "Geoffrey Irving",
            "Michael Isard",
            "Yangqing Jia",
            "Rafał Józefowicz",
            "Łukasz Kaiser",
            "Manjunath Kudlur",
            "Josh Levenberg",
            "Dan Mané",
            "Rajat Monga",
            "Sherry Moore",
            "Derek G. Murray",
            "Chris Olah",
            "Mike Schuster",
            "Jonathon Shlens",
            "Benoit Steiner",
            "Ilya Sutskever",
            "Kunal Talwar",
            "Paul A. Tucker",
            "Vincent Vanhoucke",
            "Vijay Vasudevan",
            "Fernanda Viégas",
            "Oriol Vinyals",
            "Pete Warden",
            "Martin Wattenberg",
            "Martin Wicke",
            "Yuan Yu",
            "Xiaoqiang Zheng"
        ],
        "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",
        "keywords": [
            "Heterogeneous Computing",
            "GPU Computing",
            "High-Performance Computing",
            "Simulation Platforms",
            "Parallel Computing"
        ]
    },
    {
        "title": "Ensemble Methods in Machine Learning",
        "authors": [
            "Thomas G. Dietterich"
        ],
        "abstract": "Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.",
        "keywords": [
            "Overfitting",
            "Ensemble learning",
            "Boosting (machine learning)",
            "AdaBoost",
            "Robust Learning"
        ]
    },
    {
        "title": "Machine learning: Trends, perspectives, and prospects",
        "authors": [
            "Michael I. Jordan",
            "Tom M. Mitchell"
        ],
        "abstract": "Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.",
        "keywords": [
            "Robust Learning",
            "Ensemble Learning",
            "Meta-Learning",
            "Online Learning",
            "Incremental Learning",
            "Lying"
        ]
    },
    {
        "title": "Proceedings of the 24th international conference on Machine learning",
        "authors": [
            "John Langford",
            "Joëlle Pineau"
        ],
        "abstract": "This volume contains the papers accepted to the 24th International Conference on Machine Learning (ICML 2007), which was held at Oregon State University in Corvalis, Oregon, from June 20th to 24th, 2007. ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at: http://www.machinelearning.org. This year there were 522 submissions to ICML. There was a very thorough review process, in which each paper was reviewed by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. For the first time this year there were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Also for the first time, authors were asked to submit a list of changes with their final accepted papers, which was checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 150 papers were accepted to ICML this year, including a very small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 29%. ICML attracts submissions from machine learning researchers around the globe. The 150 accepted papers this year were geographically distributed as follows: 66 papers had a first author from the US, 32 from Europe, 19 from China or Hong Kong, 11 from Canada, 6 from India, 5 each from Australia and Japan, 3 from Israel, and 1 each from Korea, Russia and Taiwan. In addition to the main program of accepted papers, which includes both a talk and poster presentation for each paper, the ICML program included 3 workshops and 8 tutorials on machine learning topics which are currently of broad interest. We were also extremely pleased to have David Heckerman (Microsoft Research), Joshua Tenenbaum (Massachussetts Institute of Technology), and Bernhard Schölkopf (Max Planck Institute for Biological Cybernetics) as the invited speakers this year. Thanks to sponsorship by the Machine Learning Journal, we were able to award a number of outstanding student paper prizes. We were fortunate this year that ICML was co-located with the International Conference on Inductive Logic Programming (ILP 2007). ICML and ILP held joint sessions on the first day of ICML 2007.",
        "keywords": [
            "Presentation (obstetrics)",
            "Support Vector Machines"
        ]
    },
    {
        "title": "Text categorization with Support Vector Machines: Learning with many relevant features",
        "authors": [
            "Thorsten Joachims"
        ],
        "abstract": "This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore they are fully automatic, eliminating the need for manual parameter tuning.",
        "keywords": [
            "Support Vector Machines (SVM)",
            "Text Categorization",
            "Semi-Supervised Learning",
            "Text Classification",
            "Multi-label Learning",
            "Empirical research"
        ]
    },
    {
        "title": "The use of the area under the ROC curve in the evaluation of machine learning algorithms",
        "authors": [
            "Andrew P. Bradley"
        ],
        "abstract": "In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six “real world” medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for “single number” evaluation of machine learning algorithms.",
        "keywords": [
            "Perceptron",
            "ROC Analysis",
            "Robust Learning",
            "Machine Learning",
            "Multilayer perceptron"
        ]
    },
    {
        "title": "TensorFlow: a system for large-scale machine learning",
        "authors": [
            "Martı́n Abadi",
            "Paul Barham",
            "Jianmin Chen",
            "Zhifeng Chen",
            "Andy Davis",
            "Jay B. Dean",
            "Matthieu Devin",
            "Sanjay Ghemawat",
            "Geoffrey Irving",
            "Michael Isard",
            "Manjunath Kudlur",
            "Josh Levenberg",
            "Rajat Monga",
            "Sherry Moore",
            "Derek G. Murray",
            "Benoit Steiner",
            "Paul A. Tucker",
            "Vijay Vasudevan",
            "Pete Warden",
            "Martin Wicke",
            "Yuan Yu",
            "Xiaoqiang Zheng"
        ],
        "abstract": "TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous parameter server designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",
        "keywords": [
            "Multi-core processor",
            "Heterogeneous Computing",
            "GPU Computing",
            "Parallel Computing",
            "High-Performance Computing",
            "Deep Learning",
            "Dataflow architecture"
        ]
    },
    {
        "title": "Proceedings of the 25th international conference on Machine learning",
        "authors": [
            "William W. Cohen",
            "Alan Yuille",
            "Sam T. Roweis"
        ],
        "abstract": "This volume contains the papers accepted to the 25th International Conference on Machine Learning (ICML 2008). ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at http://www.machinelearning.org. This year, ICML was held July 5..9 at the University of Helsinki, in Helsinki, Finland, and was co-located with COLT-2008, the 21st Annual Conference on Computational Learning Theory, and UAI-2008, the 24th Conference on Uncertainty in Artificial Intelligence. No less than 583 papers were submitted to ICML 2008. There was a very thorough review process, in which each paper was reviewed double-blind by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. There were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Some papers were checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 158 papers were accepted to ICML this year, including a small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 27%. ICML authors presented their papers both orally and in a poster session, allowing time for detailed discussions with any interested attendees of the conference. Each day of the main conference included one or two invited talks by a prominent researcher. We were very fortunate to be able to host Michael Collins, of the Massachusetts Institute of Technology; Andrew Ng, of Stanford University; and Luc De Raedt, of the Katholieke Universiteit Leuven, and John Winn of Microsoft Research Cambridge. In addition to the technical talks, ICML- 2008 also included nine tutorials held before the main conference, presented by Alex Smola, Arthur Gretton, and Kenji Fukumizu; Bert Kappen and Marc Toussaint; Neil Lawrence; MartinWainwright; Ralf Herbrich and Thore Graepel; Andreas Krause and Carlos Guestrin; Shai Shalev-Shwartz and Yoram Singer; Rob Fergus; and Matthias Seeger. This year our workshops were organized jointly with COLT and UAI as part of a special overlap day, consisting of eleven workshops selected and arranged collaboratively by the respective workshop chairs of the three conferences. This day provided a rich opportunity for interaction among the attendees of the conferences. This year, ICML enlarged its award offerings to match several other well-established conferences. We hope these will help build our community, celebrate our advances, and encourage applications and long-term thinking. In addition to our previously traditional Paper and Student Paper awards, we also gave awards for Application Paper and 10-year Best Paper (for the best paper of ICML 1998, optionally given in conjunction with a co-located conference). We thank the Machine Learning Journal for sponsoring some of our paper awards.",
        "keywords": [
            "Presentation (obstetrics)",
            "Active Learning"
        ]
    },
    {
        "title": "Pattern Recognition and Machine Learning",
        "authors": [
            "Radford M. Neal"
        ],
        "abstract": "(2007). Pattern Recognition and Machine Learning. Technometrics: Vol. 49, No. 3, pp. 366-366.",
        "keywords": [
            "Pattern Classification",
            "Backpropagation Learning",
            "Radial Basis Function Networks",
            "Deep Learning",
            "Recurrent Neural Networks"
        ]
    },
    {
        "title": "Large-Scale Machine Learning with Stochastic Gradient Descent",
        "authors": [
            "Léon Bottou"
        ],
        "abstract": "During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.",
        "keywords": [
            "Stochastic Gradient Descent",
            "Large-Scale Optimization",
            "Semi-Supervised Learning",
            "Approximation Algorithms",
            "Deep Learning",
            "Online machine learning",
            "Sample (material)"
        ]
    },
    {
        "title": "Practical Bayesian Optimization of Machine Learning Algorithms",
        "authors": [
            "Jasper Snoek",
            "Hugo Larochelle",
            "Ryan P. Adams"
        ],
        "abstract": "Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a black art that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.",
        "keywords": [
            "Hyperparameter Optimization",
            "Bayesian Optimization",
            "Robust Learning",
            "Multi-Objective Optimization",
            "Meta-Learning",
            "Optimization algorithm"
        ]
    },
    {
        "title": "Some Studies in Machine Learning Using the Game of Checkers",
        "authors": [
            "Arthur L. Samuel"
        ],
        "abstract": "Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.",
        "keywords": [
            "Player Modeling",
            "General Game Playing",
            "Computer Go"
        ]
    },
    {
        "title": "Selection of relevant features and examples in machine learning",
        "authors": [
            "Avrim Blum",
            "Pat Langley"
        ],
        "abstract": "In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples. We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area.",
        "keywords": [
            "Meta-Learning",
            "Semi-Supervised Learning",
            "Instance Selection",
            "Robust Learning"
        ]
    },
    {
        "title": "Supervised Machine Learning: A Review of Classification Techniques",
        "authors": [
            "Sotiris Kotsiantis"
        ],
        "abstract": "The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.",
        "keywords": [
            "Supervised learning",
            "One-class classification",
            "Robust Learning",
            "Support Vector Machines",
            "Meta-Learning",
            "Automated Machine Learning",
            "Cost-Sensitive Learning"
        ]
    },
    {
        "title": "Correlation-based Feature Selection for Machine Learning",
        "authors": [
            "Mark Hall"
        ],
        "abstract": "A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. This thesis addresses the problem of feature selection for machine learning through a correlation based approach. The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. A feature evaluation formula, based on ideas from test theory, provides an operational definition of this hypothesis. CFS (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy. CFS was evaluated by experiments on artificial and natural datasets. Three machine learning algorithms were used: C4.5 (a decision tree learner), IB1 (an instance based learner), and naive Bayes. Experiments on artificial datasets showed that CFS quickly identifies and screens irrelevant, redundant, and noisy features, and identifies relevant features as long as their relevance does not strongly depend on other features. On natural domains, CFS typically eliminated well over half the features. In most cases, classification accuracy using the reduced feature set equaled or bettered accuracy using the complete feature set. Feature selection degraded machine learning performance in cases where some features were eliminated which were highly predictive of very small areas of the instance space. Further experiments compared CFS with a wrapper—a well known approach to feature selection that employs the target learning algorithm to evaluate feature sets. In many cases CFS gave comparable results to the wrapper, and in general, outperformed the wrapper on small datasets. CFS executes many times faster than the wrapper, which allows it to scale to larger datasets. Two methods of extending CFS to handle feature interaction are presented and experimentally evaluated. The first considers pairs of features and the second incorporates iii feature weights calculated by the RELIEF algorithm. Experiments on artificial domains showed that both methods were able to identify interacting features. On natural domains, the pairwise method gave more reliable results than using weights provided by RELIEF.",
        "keywords": [
            "Feature (linguistics)",
            "Robust Learning",
            "Function Approximation",
            "Machine Learning",
            "Automated Machine Learning",
            "Meta-Learning",
            "Relevance (law)"
        ]
    },
    {
        "title": "Machine Learning for High-Speed Corner Detection",
        "authors": [
            "Edward Rosten",
            "Tom Drummond"
        ],
        "abstract": "Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7% of the available processing time. By comparison neither the Harris detector (120%) nor the detection stage of SIFT (300%) can operate at full frame rate. Clearly a high-speed detector is of limited use if the features produced are unsuitable for downstream processing. In particular, the same scene viewed from two different positions should yield features which correspond to the same real-world 3D locations [1]. Hence the second contribution of this paper is a comparison corner detectors based on this criterion applied to 3D scenes. This comparison supports a number of claims made elsewhere concerning existing corner detectors. Further, contrary to our initial expectations, we show that despite being principally constructed for speed, our detector significantly outperforms existing feature detectors according to this criterion.",
        "keywords": [
            "Scale-invariant feature transform",
            "Frame rate",
            "Corner detection",
            "Feature (linguistics)",
            "Feature Matching",
            "Interest Point Detectors",
            "Object Recognition",
            "Localization",
            "Deep Learning",
            "Speedup"
        ]
    },
    {
        "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
        "authors": [
            "Xiao Han",
            "Kashif Rasul",
            "Roland Vollgraf"
        ],
        "abstract": "We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist",
        "keywords": [
            "MNIST database",
            "Benchmarking",
            "Robust Learning"
        ]
    },
    {
        "title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting",
        "authors": [
            "Xingjian Shi",
            "Zhourong Chen",
            "Hao Wang",
            "Dit‐Yan Yeung",
            "Wai Kin Wong",
            "Wang‐chun Woo"
        ],
        "abstract": "The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.",
        "keywords": [
            "Nowcasting",
            "Forecasting",
            "Rainfall-Runoff Modeling",
            "Groundwater Level Forecasting",
            "Flood Inundation Modeling",
            "Probabilistic Forecasting"
        ]
    },
    {
        "title": "Gaussian Processes in Machine Learning",
        "authors": [
            "Carl Edward Rasmussen"
        ],
        "abstract": "We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.",
        "keywords": [
            "Hyperparameter",
            "Gaussian Processes",
            "Marginal likelihood",
            "Parameter Estimation"
        ]
    }
]