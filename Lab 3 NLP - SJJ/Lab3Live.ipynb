{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code done by Serden-Yilmaz Kose, Jesper Nyman and Jussi Saariniemi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"./msr_paraphrase_corpus.csv\"\n",
    "data = pd.read_csv(path, sep=\";\", header = 0, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Quality      ID1      ID2  \\\n",
      "0        1   702876   702977   \n",
      "1        0  2108705  2108831   \n",
      "2        1  1330381  1330521   \n",
      "3        0  3344667  3344648   \n",
      "4        1  1236820  1236712   \n",
      "\n",
      "                                             String1  \\\n",
      "0  Amrozi accused his brother, whom he called \"th...   \n",
      "1  Yucaipa owned Dominick's before selling the ch...   \n",
      "2  They had published an advertisement on the Int...   \n",
      "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
      "4  The stock rose $2.11, or about 11 percent, to ...   \n",
      "\n",
      "                                             String2 Unnamed: 5  \n",
      "0  Referring to him as only \"the witness\", Amrozi...        NaN  \n",
      "1  Yucaipa bought Dominick's in 1995 for $693 mil...        NaN  \n",
      "2  On June 10, the ship's owners had published an...        NaN  \n",
      "3  Tab shares jumped 20 cents, or 4.6%, to set a ...        NaN  \n",
      "4  PG&E Corp. shares jumped $1.63 or 8 percent to...        NaN  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('genesis')\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "\n",
    "def wup(S1, S2):\n",
    "    \"\"\"Wu-Palmer similarity.\"\"\"\n",
    "    return S1.wup_similarity(S2)\n",
    "\n",
    "def resnik(S1, S2):\n",
    "    \"\"\"Resnik similarity.\"\"\"\n",
    "    return S1.res_similarity(S2, genesis_ic)\n",
    "\n",
    "options = {0: wup, 1: resnik}\n",
    "\n",
    "def preProcess(sentence):\n",
    "    \"\"\"Tokenize, remove stopwords, and clean the sentence.\"\"\"\n",
    "    Stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    words = word_tokenize(sentence)\n",
    "    words = [word.lower() for word in words if word.isalpha() and word not in Stopwords]\n",
    "    return words\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character for lemmatization with WordNet.\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wn.ADJ, \"N\": wn.NOUN, \"V\": wn.VERB, \"R\": wn.ADV}\n",
    "    return tag_dict.get(tag, wn.NOUN)\n",
    "\n",
    "def word_similarity(w1, w2, num):\n",
    "    \"\"\"Calculate similarity between two words only if they share the same POS.\"\"\"\n",
    "    pos1 = get_wordnet_pos(w1)\n",
    "    pos2 = get_wordnet_pos(w2)\n",
    "\n",
    "    synsets1 = wn.synsets(w1, pos=pos1)\n",
    "    synsets2 = wn.synsets(w2, pos=pos2)\n",
    "    \n",
    "    if synsets1 and synsets2:\n",
    "        S1 = synsets1[0]\n",
    "        S2 = synsets2[0]\n",
    "        try:\n",
    "            similarity = options[num](S1, S2)\n",
    "            if similarity:\n",
    "                return round(similarity, 2)\n",
    "        except nltk.corpus.reader.wordnet.WordNetError:\n",
    "            return 0\n",
    "    return 0\n",
    "\n",
    "def Similarity(T1, T2, num):\n",
    "    \"\"\"Calculate sentence-to-sentence similarity using TF-IDF and WordNet similarity.\"\"\"\n",
    "    words1 = preProcess(T1)\n",
    "    words2 = preProcess(T2)\n",
    "\n",
    "    tf = TfidfVectorizer(use_idf=True)\n",
    "    tf.fit_transform([' '.join(words1), ' '.join(words2)])\n",
    "    \n",
    "    Idf = dict(zip(tf.get_feature_names_out(), tf.idf_))\n",
    "    \n",
    "    Sim_score1 = 0\n",
    "    Sim_score2 = 0\n",
    "\n",
    "    for w1 in words1:\n",
    "        Max = 0\n",
    "        for w2 in words2:\n",
    "            score = word_similarity(w1, w2, num)\n",
    "            if Max < score:\n",
    "                Max = score\n",
    "        Sim_score1 += Max * Idf.get(w1, 0)\n",
    "    Sim_score1 /= sum([Idf.get(w1, 0) for w1 in words1])\n",
    "\n",
    "    for w2 in words2:\n",
    "        Max = 0\n",
    "        for w1 in words1:\n",
    "            score = word_similarity(w1, w2, num)\n",
    "            if Max < score:\n",
    "                Max = score\n",
    "        Sim_score2 += Max * Idf.get(w2, 0)\n",
    "    Sim_score2 /= sum([Idf.get(w2, 0) for w2 in words2])\n",
    "\n",
    "    Sim = (Sim_score1 + Sim_score2) / 2\n",
    "    \n",
    "    return round(Sim, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Quality      ID1      ID2  \\\n",
      "0        1   702876   702977   \n",
      "1        0  2108705  2108831   \n",
      "2        1  1330381  1330521   \n",
      "3        0  3344667  3344648   \n",
      "4        1  1236820  1236712   \n",
      "\n",
      "                                             String1  \\\n",
      "0  Amrozi accused his brother, whom he called \"th...   \n",
      "1  Yucaipa owned Dominick's before selling the ch...   \n",
      "2  They had published an advertisement on the Int...   \n",
      "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
      "4  The stock rose $2.11, or about 11 percent, to ...   \n",
      "\n",
      "                                             String2 Unnamed: 5  \\\n",
      "0  Referring to him as only \"the witness\", Amrozi...        NaN   \n",
      "1  Yucaipa bought Dominick's in 1995 for $693 mil...        NaN   \n",
      "2  On June 10, the ship's owners had published an...        NaN   \n",
      "3  Tab shares jumped 20 cents, or 4.6%, to set a ...        NaN   \n",
      "4  PG&E Corp. shares jumped $1.63 or 8 percent to...        NaN   \n",
      "\n",
      "   Similarity score  \n",
      "0              0.75  \n",
      "1              0.54  \n",
      "2              0.69  \n",
      "3              0.72  \n",
      "4              0.72  \n"
     ]
    }
   ],
   "source": [
    "# Wup similarity\n",
    "\n",
    "data['Similarity score'] = 0.0\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "        T1, T2 = str(row['String1']), str(row['String2'])\n",
    "        similarity_score = Similarity(T1, T2, 0)\n",
    "        data.at[index, 'Similarity score'] = similarity_score\n",
    "print(data.head())  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the data, so we wont have to calculate the similarities again\n",
    "data.to_csv('msr_paraphrase_corpus_sim.csv', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./msr_paraphrase_corpus_sim.csv\"\n",
    "data = pd.read_csv(path, sep=\"|\", header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Quality      ID1      ID2  \\\n",
      "0        1   702876   702977   \n",
      "1        0  2108705  2108831   \n",
      "2        1  1330381  1330521   \n",
      "3        0  3344667  3344648   \n",
      "4        1  1236820  1236712   \n",
      "\n",
      "                                             String1  \\\n",
      "0  Amrozi accused his brother, whom he called \"th...   \n",
      "1  Yucaipa owned Dominick's before selling the ch...   \n",
      "2  They had published an advertisement on the Int...   \n",
      "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
      "4  The stock rose $2.11, or about 11 percent, to ...   \n",
      "\n",
      "                                             String2 Unnamed: 5  \\\n",
      "0  Referring to him as only \"the witness\", Amrozi...        NaN   \n",
      "1  Yucaipa bought Dominick's in 1995 for $693 mil...        NaN   \n",
      "2  On June 10, the ship's owners had published an...        NaN   \n",
      "3  Tab shares jumped 20 cents, or 4.6%, to set a ...        NaN   \n",
      "4  PG&E Corp. shares jumped $1.63 or 8 percent to...        NaN   \n",
      "\n",
      "   Similarity score  \n",
      "0              0.75  \n",
      "1              0.54  \n",
      "2              0.69  \n",
      "3              0.72  \n",
      "4              0.72  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the Quality and Wup similarity score:\n",
      "Pearson correlation coefficient: 0.2574210538093107\n",
      "p-value: 5.409309946120567e-61\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "cc, p = pearsonr(data['Quality'], data['Similarity score'])\n",
    "print(f\"Comparing the Quality and Wup similarity score:\")\n",
    "print(f\"Pearson correlation coefficient: {cc}\")\n",
    "print(f\"p-value: {p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Quality      ID1      ID2  \\\n",
      "0        1   702876   702977   \n",
      "1        0  2108705  2108831   \n",
      "2        1  1330381  1330521   \n",
      "3        0  3344667  3344648   \n",
      "4        1  1236820  1236712   \n",
      "\n",
      "                                             String1  \\\n",
      "0  Amrozi accused his brother, whom he called \"th...   \n",
      "1  Yucaipa owned Dominick's before selling the ch...   \n",
      "2  They had published an advertisement on the Int...   \n",
      "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
      "4  The stock rose $2.11, or about 11 percent, to ...   \n",
      "\n",
      "                                             String2 Unnamed: 5  \\\n",
      "0  Referring to him as only \"the witness\", Amrozi...        NaN   \n",
      "1  Yucaipa bought Dominick's in 1995 for $693 mil...        NaN   \n",
      "2  On June 10, the ship's owners had published an...        NaN   \n",
      "3  Tab shares jumped 20 cents, or 4.6%, to set a ...        NaN   \n",
      "4  PG&E Corp. shares jumped $1.63 or 8 percent to...        NaN   \n",
      "\n",
      "   Similarity score         Resnik  \n",
      "0              0.75  2.379404e+299  \n",
      "1              0.54  2.434157e+299  \n",
      "2              0.69  4.598166e+299  \n",
      "3              0.72   6.130000e+00  \n",
      "4              0.72  3.251332e+299  \n"
     ]
    }
   ],
   "source": [
    "data['Resnik'] = 0.0\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "        T1, T2 = str(row['String1']), str(row['String2'])\n",
    "        resnik_sim = Similarity(T1, T2, 1)\n",
    "        data.at[index, 'Resnik'] = resnik_sim\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('msr_paraphrase_corpus_sim_res.csv', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./msr_paraphrase_corpus_sim_res.csv\"\n",
    "data = pd.read_csv(path, sep=\"|\", header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the Quality and Resnik similarity score:\n",
      "Pearson correlation coefficient: 0.0850705843882259\n",
      "p-value: 8.19633795628528e-08\n"
     ]
    }
   ],
   "source": [
    "res_cc, res_p = pearsonr(data['Quality'], data['Resnik'])\n",
    "print(f\"Comparing the Quality and Resnik similarity score:\")\n",
    "print(f\"Pearson correlation coefficient: {res_cc}\")\n",
    "print(f\"p-value: {res_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between Wup and Resnik similiarity:\n",
      "     Type  Pearson correlation coefficient       p-value\n",
      "0     Wup                         0.257421  5.409310e-61\n",
      "1  Resnik                         0.085071  8.196338e-08\n"
     ]
    }
   ],
   "source": [
    "CC_table = {\n",
    "    \"Type\": [\"Wup\", \"Resnik\"],\n",
    "    \"Pearson correlation coefficient\": [cc, res_cc],\n",
    "    \"p-value\": [p, res_p]\n",
    "}\n",
    "CC_table = pd.DataFrame(CC_table)\n",
    "\n",
    "print(\"Comparison between Wup and Resnik similiarity:\")\n",
    "print(CC_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we want to use the preceding to compute threshold value beyond which a sentence-to-sentence similarity is considered as a paraphrase.\n",
    "# Suggest an approach and a script that allows you to do so by exploring the minimum value for both paraphrasing and non-paraphrasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to see how similar two sentences have to be for the similarity score to be 1\n",
    "# We should iterate through each sentence and find the lowest semantic similarity score, and make that the threshold\n",
    "\n",
    "# Import first csv sim file\n",
    "import pandas as pd\n",
    "\n",
    "path = \"./msr_paraphrase_corpus_sim_res.csv\"\n",
    "data = pd.read_csv(path, sep=\"|\", header = 0, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current maximum score: 0.54\n",
      "current maximum score: 0.72\n",
      "current maximum score: 0.74\n",
      "current maximum score: 0.81\n",
      "current maximum score: 0.88\n",
      "current maximum score: 0.89\n",
      "current maximum score: 0.9\n",
      "current maximum score: 0.92\n",
      "current maximum score: 0.95\n",
      "Non-paraphrasing threshold is 0.95.\n"
     ]
    }
   ],
   "source": [
    "# This function will find the maximum similarity score needed for the score to be 0 or \"non - paraphrased\"\n",
    "def non_paraphrasing_threshold():\n",
    "    # set the maximum score at 1, we will find lower scores later\n",
    "    maximum_sim = 0\n",
    "    # Iterate through each row\n",
    "    for index, row in data.iterrows():\n",
    "        # If the the quality score is 0, run the following\n",
    "        if row['Quality'] == 0:\n",
    "            # Make a temporary variable and assigned the similarity score to it\n",
    "            temp_sim = row['Similarity score']\n",
    "            # If the temporary similarity score is less than the current maximum score, update max score\n",
    "            if temp_sim > maximum_sim:\n",
    "                maximum_sim = temp_sim\n",
    "                print(f\"current maximum score: {maximum_sim}\")\n",
    "    # return minimum sim\n",
    "    return maximum_sim\n",
    "        \n",
    "non_threshold = non_paraphrasing_threshold()\n",
    "print(f\"Non-paraphrasing threshold is {non_threshold}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current minimum score: 0.75\n",
      "current minimum score: 0.69\n",
      "current minimum score: 0.4\n",
      "current minimum score: 0.35\n",
      "current minimum score: 0.12\n",
      "Paraphrasing threshold is 0.12\n"
     ]
    }
   ],
   "source": [
    "# This function will find the minimum similarity score needed for the score to be 1 or \"paraphrased\"\n",
    "def paraphrasing_threshold():\n",
    "    # set the minimum score at 1, we will find lower scores later\n",
    "    minimum_sim = 1\n",
    "    # Iterate through each row\n",
    "    for index, row in data.iterrows():\n",
    "        # If the the quality score is 1, run the following\n",
    "        if row['Quality'] == 1:\n",
    "            # Make a temporary variable and assigned the similarity score to it\n",
    "            temp_sim = row['Similarity score']\n",
    "            # If the temporary similarity score is less than the current minimum score, update min score\n",
    "            if temp_sim < minimum_sim:\n",
    "                minimum_sim = temp_sim\n",
    "                print(f\"current minimum score: {minimum_sim}\")\n",
    "    # return minimum sim\n",
    "    return minimum_sim\n",
    "        \n",
    "threshold = paraphrasing_threshold()\n",
    "print(f\"Paraphrasing threshold is {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we want to comprehend whether the pairs that do not match with manual labelling contain some linguistic quantifiers. \n",
    "# Suggest, a script that identifies the presence of quantifier such that negation, in the sentences and test the validity of statement \n",
    "# that “incorrect matching is often due to presence of some specific linguistic quantifiers”. You may need to manually explore the pairs \n",
    "# of sentences for which the matching between manual annotation and sentence-to-similarity score to identify those quantifiers presents \n",
    "# in such sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import first csv sim file\n",
    "import pandas as pd\n",
    "\n",
    "path = \"./msr_paraphrase_corpus_sim_res.csv\"\n",
    "data = pd.read_csv(path, sep=\"|\", header = 0, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average difference in negation value of 1 label strings is 0.030641763167725093\n"
     ]
    }
   ],
   "source": [
    "# First we need to find the threshold of negation. Meaning, whats the average difference in negation between two sentences labeled as 1?\n",
    "# https://codefather.tech/blog/nltk-python-sentiment-analysis/\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Find the negation of a string\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def negation(string):\n",
    "    scores = analyzer.polarity_scores(str(string))\n",
    "    return scores['neg']\n",
    "\n",
    "def negation_threshold():\n",
    "    # set the average negation difference at 0\n",
    "    negation_difference_average = 0\n",
    "    sentence_count = 0\n",
    "    # Iterate through each row\n",
    "    for index, row in data.iterrows():\n",
    "        # If the the quality score is 1, run the following\n",
    "        if row['Quality'] == 1:\n",
    "            # Make a temporary variable and assigned it the absolute value of the difference in negation scores\n",
    "            temp_negation = abs(negation(str(row['String1'])) - negation(str(row['String2'])))\n",
    "            # add it to the average, update sentence count\n",
    "            \"\"\"\n",
    "            if temp_negation > negation_difference_average:\n",
    "                negation_difference_average = temp_negation\n",
    "            \"\"\"\n",
    "            negation_difference_average += temp_negation\n",
    "            sentence_count += 1\n",
    "            \n",
    "            \n",
    "    return negation_difference_average / sentence_count\n",
    "    \n",
    "# store negation variable\n",
    "negation_thresh = negation_threshold()\n",
    "print(f\"Average difference in negation value of 1 label strings is {negation_thresh}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of 0 labeled sentences which reach the threshold is 1285, but only 255 have conlficting negations.\n",
      "19.844357976653697% of 0 labeled sentences which reach the threshold have conflicting negations.\n"
     ]
    }
   ],
   "source": [
    "# This function will find the negated strings among those labeled as 0\n",
    "negations_quantifiers_list = [\"not\", \"no\", \"never\", \"haven't\", \"hasn't\", \"hadn't\", \"wasn't\", \"weren't\"]\n",
    "def find_negated():\n",
    "    # create variables to count sentences, and negated sentences\n",
    "    negated_sentences = 0\n",
    "    sentences = 0\n",
    "    # Iterate through each row\n",
    "    for index, row in data.iterrows():\n",
    "        # If the label is 0 but the similarity score is above the paraphrasing threshold, do the following\n",
    "        if row['Similarity score'] >= threshold and row['Quality'] == 0:\n",
    "            # Update senteces integer\n",
    "            sentences += 1\n",
    "            # If one sentence is negated but the other isnt\n",
    "            if (negation(row['String1']) > 0 and negation(row['String2']) == 0) or (negation(row['String2']) > 0 and negation(row['String1']) == 0):\n",
    "                # update negated sentences by 1, and you may want to print the sentences\n",
    "                negated_sentences += 1\n",
    "                #print(row['String1'])\n",
    "                #print(row['String2'] + \"\\n\")\n",
    "        \n",
    "    print(f\"The number of 0 labeled sentences which reach the threshold is {sentences}, but only {negated_sentences} have conlficting negations.\")\n",
    "    print(f\"{negated_sentences * 100 / sentences}% of 0 labeled sentences which reach the threshold have conflicting negations.\")\n",
    "find_negated() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "# Load pre-trained word embeddings (Word2Vec, FastText, and GloVe) from the data folder\n",
    "word2vec = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "fasttext = fasttext.load_model('data/cc.en.300.bin')\n",
    "# glove2word2vec('data/glove.6B.300d.txt', 'data/glove.6B.300d.word2vec.txt')\n",
    "glove = KeyedVectors.load_word2vec_format('data/glove.6B.300d.txt', binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence, model):\n",
    "    words = preProcess(sentence)\n",
    "    embedding = np.mean([model[word] for word in words if word in model], axis=0)\n",
    "    return embedding\n",
    "\n",
    "def embedding_similarity(data, model):\n",
    "    similarities = []\n",
    "    for index, row in data.iterrows():\n",
    "        T1, T2 = str(row['String1']), str(row['String2'])\n",
    "        emb1 = sentence_embedding(T1, model)\n",
    "        emb2 = sentence_embedding(T2, model)\n",
    "        similarity = 1 - cosine(emb1, emb2)\n",
    "        similarities.append(similarity)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarities using Word2Vec, FastText, and GloVe\n",
    "data['Word2Vec'] = embedding_similarity(data, word2vec)\n",
    "data['FastText'] = embedding_similarity(data, fasttext)\n",
    "data['GloVe'] = embedding_similarity(data, glove)\n",
    "\n",
    "# Compute Pearson correlations for Word2Vec, FastText, and GloVe\n",
    "word2vec_cc, word2vec_p = pearsonr(data['Quality'], data['Word2Vec'])\n",
    "fasttext_cc, fasttext_p = pearsonr(data['Quality'], data['FastText'])\n",
    "glove_cc, glove_p = pearsonr(data['Quality'], data['GloVe'])\n",
    "\n",
    "print(f\"Word2Vec Pearson correlation coefficient: {word2vec_cc}, p-value: {word2vec_p}\")\n",
    "print(f\"FastText Pearson correlation coefficient: {fasttext_cc}, p-value: {fasttext_p}\")\n",
    "print(f\"GloVe Pearson correlation coefficient: {glove_cc}, p-value: {glove_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FuzzyWuzzy'] = data.apply(lambda x: fuzz.ratio(str(x['String1']), str(x['String2'])), axis=1)\n",
    "\n",
    "fuzzy_cc, fuzzy_p = pearsonr(data['Quality'], data['FuzzyWuzzy'])\n",
    "\n",
    "print(f\"FuzzyWuzzy Pearson correlation coefficient: {fuzzy_cc}, p-value: {fuzzy_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = {\n",
    "    \"Method\": [\"Word2Vec\", \"FastText\", \"GloVe\", \"FuzzyWuzzy\"],\n",
    "    \"Pearson correlation coefficient\": [word2vec_cc, fasttext_cc, glove_cc, fuzzy_cc],\n",
    "    \"p-value\": [word2vec_p, fasttext_p, glove_p, fuzzy_p]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_table)\n",
    "print(summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
