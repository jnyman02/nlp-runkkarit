{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1631759b-e72d-471c-b803-550086976831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jussi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import similar libraries to the ones included in Python script file 3\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# download nltk wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fe35b4b7-6017-48be-a843-40478a5455d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to calculate similarity of two STRINGS AND print it out\n",
    "# This will be useful since we will need to calculate many similarities\n",
    "def wu_sim_string(string1, string2):\n",
    "    # Get sysnets of given strings 1 and 2\n",
    "    sys1list = wn.synsets(string1)\n",
    "    sys2list = wn.synsets(string2)\n",
    "\n",
    "    # Run a for statements for each sysnet\n",
    "    for sys1 in sys1list:\n",
    "        for sys2 in sys2list:\n",
    "            print(f\"Similarity between {sys1} and {sys2}: {sys1.wup_similarity(sys2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dd81bce0-36bd-4dd9-ad27-e56b7fce5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, it may be more useful to calculate the wup similarity\n",
    "# if we already have the synset\n",
    "def wu_sim_syn(syn1, syn2):\n",
    "    similarity = syn1.wup_similarity(syn2)\n",
    "    #print(f\"Similarity between {syn1} and {syn2}: {similarity}\")\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9b58d100-95f9-4fe0-adb5-656f1739777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we make a function to return S1, S2, and S3 for any given 2 strings\n",
    "def calculate_similarities(string1, string2):\n",
    "    sys1list = wn.synsets(string1)\n",
    "    sys2list = wn.synsets(string2)\n",
    "    # Create emmpty similarity variables\n",
    "    S1 = 0\n",
    "    S2 = 2\n",
    "    S3 = 0\n",
    "    number_of_syn = 0\n",
    "    tmp_sim = 0\n",
    "    for sys1 in sys1list:\n",
    "        for sys2 in sys2list:\n",
    "            tmp_sim = wu_sim_syn(sys1, sys2)\n",
    "            # Update needed values, no need for if statement\n",
    "            #https://www.geeksforgeeks.org/maximum-of-two-numbers-in-python/\n",
    "            S1 = max(S1, tmp_sim)\n",
    "            S2 = min(S2, tmp_sim)\n",
    "        S3 += tmp_sim\n",
    "        number_of_syn += 1\n",
    "    # Now calculate average for S3 and return\n",
    "    S3 = S3 / number_of_syn if number_of_syn > 0 else 0\n",
    "    # print(\"\\nS1: \",S1)\n",
    "    # print(\"\\nS2: \",S2)\n",
    "    # print(\"\\nS3: \",S3)\n",
    "    return S1, S2, S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "98726d37-e4df-47f8-9de9-f7fa12120035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same same but for the first hypernym of the given words\n",
    "def calculate_hypernym_similarities(string1, string2):\n",
    "    # We need to find the firsy hypernym of the given string\n",
    "    # wn.synsets(\"dog\")[0].hypernyms()[0]\n",
    "    sys1list = wn.synsets(string1)\n",
    "    sys2list = wn.synsets(string2)\n",
    "    # Create emmpty similarity variables\n",
    "    S1 = 0\n",
    "    S2 = 2\n",
    "    S3 = 0\n",
    "    number_of_syn = 0\n",
    "    for sys1 in sys1list:\n",
    "        for sys2 in sys2list:\n",
    "            tmp_sim = wu_sim_syn(sys1.hypernyms()[0], sys2.hypernyms()[0])\n",
    "            # Update needed values, no need for if statement\n",
    "            #https://www.geeksforgeeks.org/maximum-of-two-numbers-in-python/\n",
    "            S1 = max(S1, tmp_sim)\n",
    "            S2 = min(S2, tmp_sim)\n",
    "        S3 += tmp_sim\n",
    "        number_of_syn += 1\n",
    "    # Now calculate average for S3 and return\n",
    "    S3 = S3 / number_of_syn if number_of_syn > 0 else 0\n",
    "    # print(\"\\nS1: \",S1)\n",
    "    # print(\"\\nS2: \",S2)\n",
    "    # print(\"\\nS3: \",S3)\n",
    "    return S1, S2, S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d1891238-9a62-4bf5-8e1b-4b452727e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same same but for the first hypernym of the given words\n",
    "def calculate_hypernym_similarities(string1, string2):\n",
    "    # We need to find the firsy hypernym of the given string\n",
    "    # wn.synsets(\"dog\")[0].hypernyms()[0]\n",
    "    sys1list = wn.synsets(string1)\n",
    "    sys2list = wn.synsets(string2)\n",
    "    # Create emmpty similarity variables\n",
    "    S1 = 0\n",
    "    S2 = 2\n",
    "    S3 = 0\n",
    "    number_of_syn = 0\n",
    "    for sys1 in sys1list:\n",
    "        for sys2 in sys2list:\n",
    "            tmp_sim = wu_sim_syn(sys1.hypernyms()[0], sys2.hypernyms()[0])\n",
    "            # Update needed values, no need for if statement\n",
    "            #https://www.geeksforgeeks.org/maximum-of-two-numbers-in-python/\n",
    "            S1 = max(S1, tmp_sim)\n",
    "            S2 = min(S2, tmp_sim)\n",
    "        S3 += tmp_sim\n",
    "        number_of_syn += 1\n",
    "    # Now calculate average for S3 and return\n",
    "    S3 = S3 / number_of_syn if number_of_syn > 0 else 0\n",
    "    # print(\"\\nS1: \",S1)\n",
    "    # print(\"\\nS2: \",S2)\n",
    "    # print(\"\\nS3: \",S3)\n",
    "    return S1, S2, S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c592ee2d-9d19-4198-8826-6c9750098603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same same but for the all hyponem of the given words\n",
    "def calculate_hyponym_similarities(string1, string2):\n",
    "    # We need to find the hyponym of the given string\n",
    "    sys1list = wn.synsets(string1)\n",
    "    sys2list = wn.synsets(string2)\n",
    "    # Create emmpty similarity variables\n",
    "    S1 = 0\n",
    "    S2 = 2\n",
    "    S3 = 0\n",
    "    number_of_syn = 0\n",
    "    for sys1 in sys1list:\n",
    "        for sys2 in sys2list:\n",
    "            # Now iterate for the hyponyms of given synset\n",
    "            for hypo1 in sys1.hyponyms():\n",
    "                for hypo2 in sys2.hyponyms():\n",
    "                    \n",
    "                    tmp_sim = wu_sim_syn(hypo1, hypo2)\n",
    "                    # Update needed values, no need for if statement\n",
    "                    #https://www.geeksforgeeks.org/maximum-of-two-numbers-in-python/\n",
    "                    S1 = max(S1, tmp_sim)\n",
    "                    S2 = min(S2, tmp_sim)\n",
    "                S3 += tmp_sim\n",
    "                number_of_syn += 1\n",
    "    # Now calculate average for S3 and return\n",
    "    S3 = S3 / number_of_syn\n",
    "    # print(\"\\nS1: \",S1)\n",
    "    # print(\"\\nS2: \",S2)\n",
    "    # print(\"\\nS3: \",S3)\n",
    "    return S1, S2, S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8fb9392b-03ee-4786-8044-0aadaa270748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S1:  0.96\n",
      "\n",
      "S2:  0.09523809523809523\n",
      "\n",
      "S3:  0.13357142857142856\n",
      "\n",
      "hyper1:  0.9565217391304348\n",
      "\n",
      "hyper2:  0.10526315789473684\n",
      "\n",
      "hyper3:  0.15421245421245422\n",
      "\n",
      "hypo1:  0.6666666666666666\n",
      "\n",
      "hypo2:  0.6086956521739131\n",
      "\n",
      "hypo3:  0.6238785369220107\n",
      "\n",
      "eval1:  0.96\n",
      "\n",
      "eval2:  0.6086956521739131\n",
      "\n",
      "eval3:  0.6238785369220107\n"
     ]
    }
   ],
   "source": [
    "string1 = \"car\"\n",
    "string2 = \"bus\"\n",
    "S1, S2, S3 = calculate_similarities(string1, string2)\n",
    "print(\"\\nS1: \",S1)\n",
    "print(\"\\nS2: \",S2)\n",
    "print(\"\\nS3: \",S3)\n",
    "\n",
    "hyper1, hyper2, hyper3 = calculate_hypernym_similarities(string1, string2)\n",
    "print(\"\\nhyper1: \",hyper1)\n",
    "print(\"\\nhyper2: \",hyper2)\n",
    "print(\"\\nhyper3: \",hyper3)\n",
    "\n",
    "hypo1, hypo2, hypo3 = calculate_hyponym_similarities(string1, string2)\n",
    "print(\"\\nhypo1: \",hypo1)\n",
    "print(\"\\nhypo2: \",hypo2)\n",
    "print(\"\\nhypo3: \",hypo3)\n",
    "\n",
    "eval1, eval2, eval3 = max(S1, hyper1, hypo1), max(S2, hyper2, hypo2), max(S3, hyper3, hypo3)\n",
    "print(\"\\neval1: \",eval1)\n",
    "print(\"\\neval2: \",eval2)\n",
    "print(\"\\neval3: \",eval3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f647079",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dc8210a2-0f02-47d7-8d79-21cd91fe335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\jussi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\jussi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Dowload needed corpuses\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet_ic')\n",
    "\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e67fbfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function to calculate the Jiang-Conrath similarity of two synsets\n",
    "def get_jcn_similarity(synset1, synset2):\n",
    "    # Initialize variables\n",
    "    S1 = 0\n",
    "    S2 = float('inf')\n",
    "    total = 0\n",
    "    number_of_syn = 0\n",
    "    \n",
    "    # Iterate through all synsets of both strings and calculate the similarity\n",
    "    for sys1 in synset1:\n",
    "        for sys2 in synset2:\n",
    "            tmp_sim = sys1.jcn_similarity(sys2, brown_ic)\n",
    "            S1 = max(S1, tmp_sim)\n",
    "            S2 = min(S2, tmp_sim)\n",
    "            total += tmp_sim\n",
    "            number_of_syn += 1\n",
    "    \n",
    "    # Calculate the average similarity and return all three values\n",
    "    S3 = total / number_of_syn if number_of_syn > 0 else 0\n",
    "    return S1, S2, S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2765b064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jiang-Conrath Similarity\n",
      "\n",
      "S1:  0.34659468740185323\n",
      "\n",
      "S2:  0.05161364962677664\n",
      "\n",
      "S3:  0.09387159388812355\n",
      "\n",
      "Jiang-Conrath Similarity with hypernyms\n",
      "\n",
      "S1:  0.27016908921466043\n",
      "\n",
      "S2:  0.27016908921466043\n",
      "\n",
      "S3:  0.27016908921466043\n",
      "\n",
      "Jiang-Conrath Similarity with hyponyms\n",
      "\n",
      "S1:  1e-300\n",
      "\n",
      "S2:  5e-301\n",
      "\n",
      "S3:  7.580645161290321e-301\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TASK 4\n",
    "\n",
    "# Filter synsets to only include those with the same part of speech\n",
    "synsets1 = [syn for syn in wn.synsets(string1) if syn.pos() == 'n']\n",
    "synsets2 = [syn for syn in wn.synsets(string2) if syn.pos() == 'n']\n",
    "\n",
    "S1, S2, S3 = get_jcn_similarity(synsets1, synsets2)\n",
    "print(\"\\nJiang-Conrath Similarity\")\n",
    "print(\"\\nS1: \",S1)\n",
    "print(\"\\nS2: \",S2)\n",
    "print(\"\\nS3: \",S3)\n",
    "\n",
    "S1_hyper, S2_hyper, S3_hyper = get_jcn_similarity(synsets1[0].hypernyms(), synsets2[0].hypernyms())\n",
    "print(\"\\nJiang-Conrath Similarity with hypernyms\")\n",
    "print(\"\\nS1: \",S1_hyper)\n",
    "print(\"\\nS2: \",S2_hyper)\n",
    "print(\"\\nS3: \",S3_hyper)\n",
    "\n",
    "S1_hypo, S2_hypo, S3_hypo = get_jcn_similarity(synsets1[0].hyponyms(), synsets2[0].hyponyms())\n",
    "print(\"\\nJiang-Conrath Similarity with hyponyms\")\n",
    "print(\"\\nS1: \",S1_hypo)\n",
    "print(\"\\nS2: \",S2_hypo)\n",
    "print(\"\\nS3: \",S3_hypo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c46b727",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "120d69ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jussi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jussi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "901db55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence, stopword_removal=False, stemming=False):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    if stopword_removal:\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    if stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1ec02339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit_transform(corpus)\n",
    "    return dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "\n",
    "def max_similarity(word, other_sentence_tokens):\n",
    "    word_synsets = wn.synsets(word)\n",
    "    max_sim = 0\n",
    "    for other_word in other_sentence_tokens:\n",
    "        other_word_synsets = wn.synsets(other_word)\n",
    "        if word_synsets and other_word_synsets:\n",
    "            sim = max((s1.wup_similarity(s2) or 0) for s1, s2 in product(word_synsets, other_word_synsets))\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "    return max_sim\n",
    "\n",
    "def get_sentence_similarity(tokens1, tokens2, idf):\n",
    "    def weighted_similarity(tokens1, tokens2):\n",
    "        total_sim = 0\n",
    "        total_idf = 0\n",
    "        \n",
    "        for word in tokens1:\n",
    "            max_sim = max_similarity(word, tokens2)\n",
    "            word_idf = idf.get(word, 0)\n",
    "            total_sim += max_sim * word_idf\n",
    "            total_idf += word_idf\n",
    "        return total_sim / total_idf if total_idf > 0 else 0\n",
    "    \n",
    "    sim_T1_T2 = weighted_similarity(tokens1, tokens2)\n",
    "    sim_T2_T1 = weighted_similarity(tokens2, tokens1)\n",
    "    \n",
    "    return 0.5 * (sim_T1_T2 + sim_T2_T1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2e87d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Similarity without preprocessing:  0.6791713600199532\n",
      "\n",
      "Tokens 1: ['students', 'feel', 'unhappy', 'today', 'about', 'the', 'class', 'today']\n",
      "\n",
      "Tokens 2: ['several', 'students', 'study', 'hard', 'at', 'classes', 'in', 'recent', 'days']\n",
      "\n",
      "Sentence Similarity with stopword removal:  0.7733014562931559\n",
      "\n",
      "Tokens 1: ['students', 'feel', 'unhappy', 'today', 'class', 'today']\n",
      "\n",
      "Tokens 2: ['several', 'students', 'study', 'hard', 'classes', 'recent', 'days']\n",
      "\n",
      "Sentence Similarity with stopword removal and stemming:  0.7730186480186481\n",
      "\n",
      "Tokens 1: ['student', 'feel', 'unhappi', 'today', 'class', 'today']\n",
      "\n",
      "Tokens 2: ['sever', 'student', 'studi', 'hard', 'class', 'recent', 'day']\n"
     ]
    }
   ],
   "source": [
    "T1 = \"Students feel unhappy today about the class today.\"\n",
    "T2 = \"Several students study hard at classes in recent days.\"\n",
    "\n",
    "tokens1 = preprocess(T1)\n",
    "tokens2 = preprocess(T2)\n",
    "idf = compute_idf([T1, T2])\n",
    "\n",
    "print(\"\\nSentence Similarity without preprocessing: \", get_sentence_similarity(tokens1, tokens2, idf))\n",
    "print(\"\\nTokens 1:\", tokens1)\n",
    "print(\"\\nTokens 2:\", tokens2)\n",
    "\n",
    "tokens1 = preprocess(T1, stopword_removal=True)\n",
    "tokens2 = preprocess(T2, stopword_removal=True)\n",
    "idf = compute_idf([T1, T2])\n",
    "\n",
    "print(\"\\nSentence Similarity with stopword removal: \", get_sentence_similarity(tokens1, tokens2, idf))\n",
    "print(\"\\nTokens 1:\", tokens1)\n",
    "print(\"\\nTokens 2:\", tokens2)\n",
    "\n",
    "tokens1 = preprocess(T1, stopword_removal=True, stemming=True)\n",
    "tokens2 = preprocess(T2, stopword_removal=True, stemming=True)\n",
    "idf = compute_idf([T1, T2])\n",
    "\n",
    "print(\"\\nSentence Similarity with stopword removal and stemming: \", get_sentence_similarity(tokens1, tokens2, idf))\n",
    "print(\"\\nTokens 1:\", tokens1)\n",
    "print(\"\\nTokens 2:\", tokens2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6453ad",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "efaf13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_noun(word):\n",
    "    noun_form = wn.morphy(word, wn.NOUN)\n",
    "    return noun_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "23da24eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nouns in T1: ['student', 'feel', 'today', 'class', 'today']\n",
      "\n",
      "Nouns in T2: ['student', 'study', 'class', 'recent', 'days']\n",
      "\n",
      "Sentence Similarity with only nouns:  0.8301128695865538\n"
     ]
    }
   ],
   "source": [
    "T1_tokens = preprocess(T1, stopword_removal=True)\n",
    "T2_tokens = preprocess(T2, stopword_removal=True)\n",
    "\n",
    "T1_nouns = [word for word in [to_noun(word) for word in T1_tokens] if word]\n",
    "T2_nouns = [word for word in [to_noun(word) for word in T2_tokens] if word]\n",
    "\n",
    "print(\"\\nNouns in T1:\", T1_nouns)\n",
    "print(\"\\nNouns in T2:\", T2_nouns)\n",
    "\n",
    "idf = compute_idf([T1, T2])\n",
    "\n",
    "similarity_with_nouns = get_sentence_similarity(T1_nouns, T2_nouns, idf)\n",
    "\n",
    "print(\"\\nSentence Similarity with only nouns: \", similarity_with_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dfa2e5",
   "metadata": {},
   "source": [
    "Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9432bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "648323b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "\n",
    "def embedding_vectors(T, model, type):\n",
    "    tokens = T.split()\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if type == 'fasttext':\n",
    "            vectors.append(model.get_word_vector(token))\n",
    "        elif type == 'word2vec':\n",
    "            vectors.append(model.wv[token])\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8e968ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FastText Similarity:  0.5952126\n",
      "\n",
      "Word2Vec Similarity:  -0.046015095\n",
      "\n",
      "Doc2Vec Similarity:  0.17326224\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = fasttext.load_model('cc.en.300.bin')\n",
    "#word2vec_model = Word2Vec(brown.sents(), min_count=1)\n",
    "\n",
    "word2vec_model = Word2Vec([T1.split(), T2.split()], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate([T1.split(), T2.split()])]\n",
    "doc2vec_model = Doc2Vec(documents, vector_size=100, window=2, min_count=1, workers=4)\n",
    "\n",
    "emb1 = embedding_vectors(T1, fasttext_model, \"fasttext\")\n",
    "emb2 = embedding_vectors(T2, fasttext_model, \"fasttext\")\n",
    "\n",
    "similarity = cosine_similarity(emb1, emb2)\n",
    "print(\"\\nFastText Similarity: \", similarity)\n",
    "\n",
    "emb1 = embedding_vectors(T1, word2vec_model, \"word2vec\")\n",
    "emb2 = embedding_vectors(T2, word2vec_model, \"word2vec\")\n",
    "\n",
    "similarity = cosine_similarity(emb1, emb2)\n",
    "print(\"\\nWord2Vec Similarity: \", similarity)\n",
    "\n",
    "emb1 = doc2vec_model.infer_vector(T1.split())\n",
    "emb2 = doc2vec_model.infer_vector(T2.split())\n",
    "\n",
    "similarity = cosine_similarity(emb1, emb2)\n",
    "print(\"\\nDoc2Vec Similarity: \", similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c188b2",
   "metadata": {},
   "source": [
    "Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2d9dbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "split_T1 = T1.lower().split(\" \")\n",
    "split_T1 = [wnl.lemmatize(word.strip(string.punctuation)) for word in split_T1 if word.strip(string.punctuation).isalpha()]\n",
    "\n",
    "split_T2 = T2.lower().split(\" \")\n",
    "split_T2 = [wnl.lemmatize(word.strip(string.punctuation)) for word in split_T2 if word.strip(string.punctuation).isalpha()]\n",
    "\n",
    "preprocessed_T1 = \" \".join(split_T1)\n",
    "preprocessed_T2 = \" \".join(split_T2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "61438912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FuzzyWuzzy Similarity:  54\n"
     ]
    }
   ],
   "source": [
    "def fuzzyWuzzySimilarity(T1, T2):\n",
    "    return fuzz.ratio(T1, T2)\n",
    "\n",
    "similarity = fuzzyWuzzySimilarity(preprocessed_T1, preprocessed_T2)\n",
    "print(\"\\nFuzzyWuzzy Similarity: \", similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
