{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1631759b-e72d-471c-b803-550086976831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jesper\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import similar libraries to the ones included in Python script file 3\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# download nltk wordnet\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fe35b4b7-6017-48be-a843-40478a5455d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function to calculate similarity of two STRINGS AND print it out\n",
    "# This will be useful since we will need to calculate many similarities\n",
    "def wu_sim_string(string1, string2):\n",
    "    # Get sysnets of given strings 1 and 2\n",
    "    sys1list = wn.synsets(string1)\n",
    "    sys2list = wn.synsets(string2)\n",
    "\n",
    "    # Run a for statements for each sysnet\n",
    "    for sys1 in sys1list:\n",
    "        for sys2 in sys2list:\n",
    "            print(f\"Similarity between {sys1} and {sys2}: {sys1.wup_similarity(sys2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dd81bce0-36bd-4dd9-ad27-e56b7fce5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, it may be more useful to calculate the wup similarity\n",
    "# if we already have the synset\n",
    "def wu_sim_syn(syn1, syn2):\n",
    "    similarity = syn1.wup_similarity(syn2)\n",
    "    #print(f\"Similarity between {syn1} and {syn2}: {similarity}\")\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9b58d100-95f9-4fe0-adb5-656f1739777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we make a function to return S1, S2, and S3 for any given 2 strings\n",
    "def calculate_similarities(string1, string2):\n",
    "    sys1list = wn.synsets(string1)\n",
    "    sys2list = wn.synsets(string2)\n",
    "    # Create emmpty similarity variables\n",
    "    S1 = 0\n",
    "    S2 = 2\n",
    "    S3 = 0\n",
    "    number_of_syn = 0\n",
    "    tmp_sim = 0\n",
    "    for sys1 in sys1list:\n",
    "        for sys2 in sys2list:\n",
    "            tmp_sim = wu_sim_syn(sys1, sys2)\n",
    "            # Update needed values, no need for if statement\n",
    "            #https://www.geeksforgeeks.org/maximum-of-two-numbers-in-python/\n",
    "            S1 = max(S1, tmp_sim)\n",
    "            S2 = min(S2, tmp_sim)\n",
    "        S3 += tmp_sim\n",
    "        number_of_syn += 1\n",
    "    # Now calculate average for S3 and return\n",
    "    S3 = S3 / number_of_syn if number_of_syn > 0 else 0\n",
    "    # print(\"\\nS1: \",S1)\n",
    "    # print(\"\\nS2: \",S2)\n",
    "    # print(\"\\nS3: \",S3)\n",
    "    return S1, S2, S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "98726d37-e4df-47f8-9de9-f7fa12120035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same same but for the first hypernym of the given words\n",
    "def calculate_hypernym_similarities(string1, string2):\n",
    "    # We need to find the firsy hypernym of the given string\n",
    "    # wn.synsets(\"dog\")[0].hypernyms()[0]\n",
    "    sys1list = wn.synsets(string1)\n",
    "    sys2list = wn.synsets(string2)\n",
    "    # Create emmpty similarity variables\n",
    "    S1 = 0\n",
    "    S2 = 2\n",
    "    S3 = 0\n",
    "    number_of_syn = 0\n",
    "    for sys1 in sys1list:\n",
    "        for sys2 in sys2list:\n",
    "            tmp_sim = wu_sim_syn(sys1.hypernyms()[0], sys2.hypernyms()[0])\n",
    "            # Update needed values, no need for if statement\n",
    "            #https://www.geeksforgeeks.org/maximum-of-two-numbers-in-python/\n",
    "            S1 = max(S1, tmp_sim)\n",
    "            S2 = min(S2, tmp_sim)\n",
    "        S3 += tmp_sim\n",
    "        number_of_syn += 1\n",
    "    # Now calculate average for S3 and return\n",
    "    S3 = S3 / number_of_syn if number_of_syn > 0 else 0\n",
    "    # print(\"\\nS1: \",S1)\n",
    "    # print(\"\\nS2: \",S2)\n",
    "    # print(\"\\nS3: \",S3)\n",
    "    return S1, S2, S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d1891238-9a62-4bf5-8e1b-4b452727e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same same but for the first hypernym of the given words\n",
    "def calculate_hypernym_similarities(string1, string2):\n",
    "    # We need to find the firsy hypernym of the given string\n",
    "    # wn.synsets(\"dog\")[0].hypernyms()[0]\n",
    "    sys1list = wn.synsets(string1)\n",
    "    sys2list = wn.synsets(string2)\n",
    "    # Create emmpty similarity variables\n",
    "    S1 = 0\n",
    "    S2 = 2\n",
    "    S3 = 0\n",
    "    number_of_syn = 0\n",
    "    for sys1 in sys1list:\n",
    "        for sys2 in sys2list:\n",
    "            tmp_sim = wu_sim_syn(sys1.hypernyms()[0], sys2.hypernyms()[0])\n",
    "            # Update needed values, no need for if statement\n",
    "            #https://www.geeksforgeeks.org/maximum-of-two-numbers-in-python/\n",
    "            S1 = max(S1, tmp_sim)\n",
    "            S2 = min(S2, tmp_sim)\n",
    "        S3 += tmp_sim\n",
    "        number_of_syn += 1\n",
    "    # Now calculate average for S3 and return\n",
    "    S3 = S3 / number_of_syn if number_of_syn > 0 else 0\n",
    "    # print(\"\\nS1: \",S1)\n",
    "    # print(\"\\nS2: \",S2)\n",
    "    # print(\"\\nS3: \",S3)\n",
    "    return S1, S2, S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c592ee2d-9d19-4198-8826-6c9750098603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same same but for the all hyponem of the given words\n",
    "def calculate_hyponym_similarities(string1, string2):\n",
    "    # We need to find the hyponym of the given string\n",
    "    sys1list = wn.synsets(string1)\n",
    "    sys2list = wn.synsets(string2)\n",
    "    # Create emmpty similarity variables\n",
    "    S1 = 0\n",
    "    S2 = 2\n",
    "    S3 = 0\n",
    "    number_of_syn = 0\n",
    "    for sys1 in sys1list:\n",
    "        for sys2 in sys2list:\n",
    "            # Now iterate for the hyponyms of given synset\n",
    "            for hypo1 in sys1.hyponyms():\n",
    "                for hypo2 in sys2.hyponyms():\n",
    "                    \n",
    "                    tmp_sim = wu_sim_syn(hypo1, hypo2)\n",
    "                    # Update needed values, no need for if statement\n",
    "                    #https://www.geeksforgeeks.org/maximum-of-two-numbers-in-python/\n",
    "                    S1 = max(S1, tmp_sim)\n",
    "                    S2 = min(S2, tmp_sim)\n",
    "                S3 += tmp_sim\n",
    "                number_of_syn += 1\n",
    "    # Now calculate average for S3 and return\n",
    "    S3 = S3 / number_of_syn\n",
    "    # print(\"\\nS1: \",S1)\n",
    "    # print(\"\\nS2: \",S2)\n",
    "    # print(\"\\nS3: \",S3)\n",
    "    return S1, S2, S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8fb9392b-03ee-4786-8044-0aadaa270748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S1:  0.96\n",
      "\n",
      "S2:  0.09523809523809523\n",
      "\n",
      "S3:  0.13357142857142856\n",
      "\n",
      "hyper1:  0.9565217391304348\n",
      "\n",
      "hyper2:  0.10526315789473684\n",
      "\n",
      "hyper3:  0.15421245421245422\n",
      "\n",
      "hypo1:  0.6666666666666666\n",
      "\n",
      "hypo2:  0.6086956521739131\n",
      "\n",
      "hypo3:  0.6238785369220107\n",
      "\n",
      "eval1:  0.96\n",
      "\n",
      "eval2:  0.6086956521739131\n",
      "\n",
      "eval3:  0.6238785369220107\n"
     ]
    }
   ],
   "source": [
    "string1 = \"car\"\n",
    "string2 = \"bus\"\n",
    "S1, S2, S3 = calculate_similarities(string1, string2)\n",
    "print(\"\\nS1: \",S1)\n",
    "print(\"\\nS2: \",S2)\n",
    "print(\"\\nS3: \",S3)\n",
    "\n",
    "hyper1, hyper2, hyper3 = calculate_hypernym_similarities(string1, string2)\n",
    "print(\"\\nhyper1: \",hyper1)\n",
    "print(\"\\nhyper2: \",hyper2)\n",
    "print(\"\\nhyper3: \",hyper3)\n",
    "\n",
    "hypo1, hypo2, hypo3 = calculate_hyponym_similarities(string1, string2)\n",
    "print(\"\\nhypo1: \",hypo1)\n",
    "print(\"\\nhypo2: \",hypo2)\n",
    "print(\"\\nhypo3: \",hypo3)\n",
    "\n",
    "eval1, eval2, eval3 = max(S1, hyper1, hypo1), max(S2, hyper2, hypo2), max(S3, hyper3, hypo3)\n",
    "print(\"\\neval1: \",eval1)\n",
    "print(\"\\neval2: \",eval2)\n",
    "print(\"\\neval3: \",eval3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f647079",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dc8210a2-0f02-47d7-8d79-21cd91fe335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Jesper\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\Jesper\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Dowload needed corpuses\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet_ic')\n",
    "\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e67fbfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function to calculate the Jiang-Conrath similarity of two synsets\n",
    "def get_jcn_similarity(synset1, synset2):\n",
    "    # Initialize variables\n",
    "    S1 = 0\n",
    "    S2 = float('inf')\n",
    "    total = 0\n",
    "    number_of_syn = 0\n",
    "    \n",
    "    # Iterate through all synsets of both strings and calculate the similarity\n",
    "    for sys1 in synset1:\n",
    "        for sys2 in synset2:\n",
    "            tmp_sim = sys1.jcn_similarity(sys2, brown_ic)\n",
    "            S1 = max(S1, tmp_sim)\n",
    "            S2 = min(S2, tmp_sim)\n",
    "            total += tmp_sim\n",
    "            number_of_syn += 1\n",
    "    \n",
    "    # Calculate the average similarity and return all three values\n",
    "    S3 = total / number_of_syn if number_of_syn > 0 else 0\n",
    "    return S1, S2, S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2765b064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jiang-Conrath Similarity\n",
      "\n",
      "S1:  0.34659468740185323\n",
      "\n",
      "S2:  0.05161364962677664\n",
      "\n",
      "S3:  0.09387159388812355\n",
      "\n",
      "Jiang-Conrath Similarity with hypernyms\n",
      "\n",
      "S1:  0.27016908921466043\n",
      "\n",
      "S2:  0.27016908921466043\n",
      "\n",
      "S3:  0.27016908921466043\n",
      "\n",
      "Jiang-Conrath Similarity with hyponyms\n",
      "\n",
      "S1:  1e-300\n",
      "\n",
      "S2:  5e-301\n",
      "\n",
      "S3:  7.580645161290321e-301\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TASK 4\n",
    "\n",
    "# Filter synsets to only include those with the same part of speech\n",
    "synsets1 = [syn for syn in wn.synsets(string1) if syn.pos() == 'n']\n",
    "synsets2 = [syn for syn in wn.synsets(string2) if syn.pos() == 'n']\n",
    "\n",
    "S1, S2, S3 = get_jcn_similarity(synsets1, synsets2)\n",
    "print(\"\\nJiang-Conrath Similarity\")\n",
    "print(\"\\nS1: \",S1)\n",
    "print(\"\\nS2: \",S2)\n",
    "print(\"\\nS3: \",S3)\n",
    "\n",
    "S1_hyper, S2_hyper, S3_hyper = get_jcn_similarity(synsets1[0].hypernyms(), synsets2[0].hypernyms())\n",
    "print(\"\\nJiang-Conrath Similarity with hypernyms\")\n",
    "print(\"\\nS1: \",S1_hyper)\n",
    "print(\"\\nS2: \",S2_hyper)\n",
    "print(\"\\nS3: \",S3_hyper)\n",
    "\n",
    "S1_hypo, S2_hypo, S3_hypo = get_jcn_similarity(synsets1[0].hyponyms(), synsets2[0].hyponyms())\n",
    "print(\"\\nJiang-Conrath Similarity with hyponyms\")\n",
    "print(\"\\nS1: \",S1_hypo)\n",
    "print(\"\\nS2: \",S2_hypo)\n",
    "print(\"\\nS3: \",S3_hypo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c46b727",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "120d69ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jesper\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jesper\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "901db55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence, sropword_removal=False, stemming=False):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    if sropword_removal:\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    if stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1ec02339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_similarity(tokens1, tokens2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform([tokens1, tokens2])\n",
    "    return cosine_similarity(tfidf)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2e87d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Similarity without preprocessing:  0.05629715757507138\n",
      "\n",
      "Sentence Similarity with stopword removal:  0.07244081925041986\n",
      "\n",
      "Sentence Similarity with stopword removal and stemming:  0.15592892548708365\n"
     ]
    }
   ],
   "source": [
    "T1 = \"Students feel unhappy today about the class today.\"\n",
    "T2 = \"Several students study hard at classes in recent days.\"\n",
    "\n",
    "tokens1 = preprocess(T1)\n",
    "tokens2 = preprocess(T2)\n",
    "\n",
    "print(\"\\nSentence Similarity without preprocessing: \", get_sentence_similarity(tokens1, tokens2))\n",
    "\n",
    "tokens1 = preprocess(T1, sropword_removal=True)\n",
    "tokens2 = preprocess(T2, sropword_removal=True)\n",
    "\n",
    "print(\"\\nSentence Similarity with stopword removal: \", get_sentence_similarity(tokens1, tokens2))\n",
    "\n",
    "tokens1 = preprocess(T1, sropword_removal=True, stemming=True)\n",
    "tokens2 = preprocess(T2, sropword_removal=True, stemming=True)\n",
    "\n",
    "print(\"\\nSentence Similarity with stopword removal and stemming: \", get_sentence_similarity(tokens1, tokens2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
